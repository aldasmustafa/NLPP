{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doğal Dil İşleme Dersi - Ödev 1: Veri Hazırlama\n",
    "\n",
    "Bu notebook, metin tabanlı bir veri setinin indirilmesi ve hazırlanması adımlarını içermektedir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gerekli Kütüphanelerin Yüklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Grafik ayarları\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Veri Setinin İndirilmesi\n",
    "\n",
    "Bu bölümde, çalışmak için seçtiğiniz veri setini indirme işlemini gerçekleştireceksiniz. Veri setinizi seçerken metin tabanlı ve yeterli büyüklükte olmasına dikkat edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setinin indirileceği URL\n",
    "data_url = \"\" # Veri seti URL'sini buraya ekleyin\n",
    "\n",
    "# Veri setini indirme işlemi\n",
    "# Örnek: Kaggle API kullanımı veya doğrudan indirme\n",
    "\n",
    "# Veri setinin kaydedileceği dizin\n",
    "raw_data_dir = \"../data/raw/\"\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "\n",
    "# Veri setini indirme ve kaydetme\n",
    "# Örnek kod (URL'den indirme):\n",
    "'''\n",
    "response = requests.get(data_url)\n",
    "with open(os.path.join(raw_data_dir, \"dataset.json\"), \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Veri Setinin İncelenmesi\n",
    "\n",
    "İndirilen veri setinin yapısını ve içeriğini inceleyeceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini yükleme (format örneği: JSON)\n",
    "'''\n",
    "with open(os.path.join(raw_data_dir, \"dataset.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "'''\n",
    "\n",
    "# Veri seti hakkında genel bilgiler\n",
    "# Örnek: Toplam döküman sayısı, boyut, format bilgisi\n",
    "'''\n",
    "print(f\"Toplam döküman sayısı: {len(data)}\")\n",
    "print(f\"Veri seti boyutu: {os.path.getsize(os.path.join(raw_data_dir, 'dataset.json')) / (1024*1024):.2f} MB\")\n",
    "print(f\"Veri seti formatı: JSON\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Veri Setinden Örnek İçerik\n",
    "\n",
    "Veri setinden örnek bir parça gösterelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek veri gösterimi\n",
    "'''\n",
    "# İlk 5 örneği göster\n",
    "for i, example in enumerate(data[:5]):\n",
    "    print(f\"Örnek {i+1}:\")\n",
    "    print(example)\n",
    "    print(\"-\" * 50)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ham Verinin Metin Formatına Dönüştürülmesi\n",
    "\n",
    "Veri setini metin işleme için uygun formata dönüştürelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metin verilerini çıkarma\n",
    "'''\n",
    "texts = []\n",
    "for item in data:\n",
    "    # Veri setine göre metin alanını çıkarma\n",
    "    # Örnek: item[\"text\"] veya item[\"content\"]\n",
    "    text = item[\"text\"]  # Veri setinize göre değiştirin\n",
    "    texts.append(text)\n",
    "'''\n",
    "\n",
    "# Metinleri tek bir dosyaya kaydetme\n",
    "'''\n",
    "with open(os.path.join(raw_data_dir, \"raw_texts.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in texts:\n",
    "        f.write(text + \"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zipf Yasası Analizi (Ham Veri)\n",
    "\n",
    "Ham veri üzerinde Zipf yasası analizini gerçekleştirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm metinleri birleştirme\n",
    "'''\n",
    "all_text = \" \".join(texts)\n",
    "'''\n",
    "\n",
    "# Kelimelere ayırma (basit tokenization)\n",
    "'''\n",
    "words = re.findall(r'\\w+', all_text.lower())\n",
    "'''\n",
    "\n",
    "# Kelime frekanslarını hesaplama\n",
    "'''\n",
    "word_counts = Counter(words)\n",
    "'''\n",
    "\n",
    "# En sık kullanılan 50 kelimeyi gösterme\n",
    "'''\n",
    "print(\"En sık kullanılan 50 kelime:\")\n",
    "for word, count in word_counts.most_common(50):\n",
    "    print(f\"{word}: {count}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zipf yasası grafiği için verileri hazırlama\n",
    "'''\n",
    "word_freq = [(word, count) for word, count in word_counts.items()]\n",
    "word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "ranks = np.arange(1, len(word_freq) + 1)\n",
    "frequencies = np.array([freq for word, freq in word_freq])\n",
    "'''\n",
    "\n",
    "# Log-log grafiği çizme\n",
    "'''\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.loglog(ranks, frequencies, 'b.')\n",
    "plt.xlabel('Kelime Sıralaması (log)', fontsize=14)\n",
    "plt.ylabel('Kelime Frekansı (log)', fontsize=14)\n",
    "plt.title('Zipf Yasası Analizi (Ham Veri)', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zipf yasası eğrisi (1/rank ilişkisi)\n",
    "plt.loglog(ranks, frequencies[0] / ranks, 'r-', label='Zipf Yasası (1/rank)')\n",
    "plt.legend()\n",
    "plt.savefig('../data/raw/zipf_raw_data.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Veri Seti Yeterliliğinin Değerlendirilmesi\n",
    "\n",
    "Veri setinin boyut ve içerik açısından yeterliliğini değerlendirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri seti istatistikleri\n",
    "'''\n",
    "total_words = len(words)\n",
    "unique_words = len(word_counts)\n",
    "vocabulary_richness = unique_words / total_words\n",
    "\n",
    "print(f\"Toplam kelime sayısı: {total_words}\")\n",
    "print(f\"Benzersiz kelime sayısı: {unique_words}\")\n",
    "print(f\"Kelime çeşitliliği oranı: {vocabulary_richness:.4f}\")\n",
    "'''\n",
    "\n",
    "# Veri seti yeterliliği değerlendirmesi\n",
    "'''\n",
    "if total_words > 100000 and unique_words > 10000:\n",
    "    print(\"Veri seti boyut olarak yeterlidir.\")\n",
    "else:\n",
    "    print(\"Veri seti boyut olarak yetersiz olabilir.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sonuç\n",
    "\n",
    "Bu notebook'ta veri setini indirdik, inceledik ve ham veri üzerinde Zipf yasası analizini gerçekleştirdik. Bir sonraki adımda, veri setine ön işleme adımlarını uygulayacağız."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
