{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doğal Dil İşleme Dersi - Ödev 1: Vektörleştirme (Vectorization)\n",
    "\n",
    "Bu notebook, ön işleme adımlarından geçirilmiş metin verilerinin TF-IDF ve Word2Vec yöntemleriyle vektörleştirilmesini içermektedir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gerekli Kütüphanelerin Yüklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Grafik ayarları\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. İşlenmiş Verilerin Yüklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İşlenmiş verileri yükleme\n",
    "processed_data_dir = \"../data/processed/\"\n",
    "models_dir = \"../models/\"\n",
    "\n",
    "# Dizinlerin var olduğundan emin olalım\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Stemming ve lemmatization sonucu elde edilen verileri yükleme\n",
    "'''\n",
    "stemmed_df = pd.read_csv(os.path.join(processed_data_dir, \"stemmed_data.csv\"))\n",
    "lemmatized_df = pd.read_csv(os.path.join(processed_data_dir, \"lemmatized_data.csv\"))\n",
    "\n",
    "print(f\"Stemming sonrası veri boyutu: {stemmed_df.shape}\")\n",
    "print(f\"Lemmatization sonrası veri boyutu: {lemmatized_df.shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Vektörleştirme\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency), bir kelimenin bir belgede ne kadar önemli olduğunu ölçen istatistiksel bir yöntemdir. Bu bölümde, hem stemming hem de lemmatization sonucu elde edilen veriler için TF-IDF vektörleştirme işlemini gerçekleştireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonrası veriler için TF-IDF vektörleştirme\n",
    "'''\n",
    "# TF-IDF vektörleştirici oluşturma\n",
    "tfidf_vectorizer_stemmed = TfidfVectorizer(max_features=5000)  # En sık kullanılan 5000 kelimeyi kullan\n",
    "\n",
    "# Vektörleştirme işlemini gerçekleştirme\n",
    "tfidf_matrix_stemmed = tfidf_vectorizer_stemmed.fit_transform(stemmed_df['text'])\n",
    "\n",
    "# Özellikleri (kelimeleri) alma\n",
    "feature_names_stemmed = tfidf_vectorizer_stemmed.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini DataFrame'e dönüştürme\n",
    "tfidf_df_stemmed = pd.DataFrame(tfidf_matrix_stemmed.toarray(), columns=feature_names_stemmed)\n",
    "tfidf_df_stemmed.index = stemmed_df['document_id']\n",
    "\n",
    "# TF-IDF DataFrame'ini kaydetme\n",
    "tfidf_df_stemmed.to_csv(os.path.join(processed_data_dir, \"tfidf_stemmed.csv\"))\n",
    "\n",
    "print(f\"Stemming sonrası TF-IDF matris boyutu: {tfidf_df_stemmed.shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization sonrası veriler için TF-IDF vektörleştirme\n",
    "'''\n",
    "# TF-IDF vektörleştirici oluşturma\n",
    "tfidf_vectorizer_lemmatized = TfidfVectorizer(max_features=5000)  # En sık kullanılan 5000 kelimeyi kullan\n",
    "\n",
    "# Vektörleştirme işlemini gerçekleştirme\n",
    "tfidf_matrix_lemmatized = tfidf_vectorizer_lemmatized.fit_transform(lemmatized_df['text'])\n",
    "\n",
    "# Özellikleri (kelimeleri) alma\n",
    "feature_names_lemmatized = tfidf_vectorizer_lemmatized.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini DataFrame'e dönüştürme\n",
    "tfidf_df_lemmatized = pd.DataFrame(tfidf_matrix_lemmatized.toarray(), columns=feature_names_lemmatized)\n",
    "tfidf_df_lemmatized.index = lemmatized_df['document_id']\n",
    "\n",
    "# TF-IDF DataFrame'ini kaydetme\n",
    "tfidf_df_lemmatized.to_csv(os.path.join(processed_data_dir, \"tfidf_lemmatized.csv\"))\n",
    "\n",
    "print(f\"Lemmatization sonrası TF-IDF matris boyutu: {tfidf_df_lemmatized.shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF Sonuçlarının İncelenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonrası TF-IDF matrisinden ilk 5 satır ve 10 sütunu gösterme\n",
    "'''\n",
    "print(\"Stemming sonrası TF-IDF matrisinden örnek:\")\n",
    "tfidf_df_stemmed.iloc[:5, :10]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization sonrası TF-IDF matrisinden ilk 5 satır ve 10 sütunu gösterme\n",
    "'''\n",
    "print(\"Lemmatization sonrası TF-IDF matrisinden örnek:\")\n",
    "tfidf_df_lemmatized.iloc[:5, :10]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonrası en yüksek TF-IDF değerine sahip kelimeleri gösterme\n",
    "'''\n",
    "# Her belge için en yüksek TF-IDF değerine sahip 5 kelimeyi bulma\n",
    "def get_top_tfidf_words(tfidf_df, n=5):\n",
    "    results = []\n",
    "    for i in range(len(tfidf_df)):\n",
    "        row = tfidf_df.iloc[i]\n",
    "        top_indices = row.nlargest(n).index\n",
    "        top_values = row.nlargest(n).values\n",
    "        results.append({\n",
    "            'document_id': tfidf_df.index[i],\n",
    "            'top_words': [(word, value) for word, value in zip(top_indices, top_values)]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "top_stemmed_words = get_top_tfidf_words(tfidf_df_stemmed)\n",
    "\n",
    "# İlk 5 belge için en önemli kelimeleri gösterme\n",
    "for i, result in enumerate(top_stemmed_words[:5]):\n",
    "    print(f\"Belge {result['document_id']} için en önemli kelimeler:\")\n",
    "    for word, value in result['top_words']:\n",
    "        print(f\"  {word}: {value:.4f}\")\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization sonrası en yüksek TF-IDF değerine sahip kelimeleri gösterme\n",
    "'''\n",
    "top_lemmatized_words = get_top_tfidf_words(tfidf_df_lemmatized)\n",
    "\n",
    "# İlk 5 belge için en önemli kelimeleri gösterme\n",
    "for i, result in enumerate(top_lemmatized_words[:5]):\n",
    "    print(f\"Belge {result['document_id']} için en önemli kelimeler:\")\n",
    "    for word, value in result['top_words']:\n",
    "        print(f\"  {word}: {value:.4f}\")\n",
    "    print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word2Vec Vektörleştirme\n",
    "\n",
    "Word2Vec, kelimeleri anlamsal olarak benzer vektörlere dönüştüren bir yöntemdir. Bu bölümde, hem stemming hem de lemmatization sonucu elde edilen veriler için farklı parametrelerle Word2Vec modelleri eğiteceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec için verileri hazırlama\n",
    "'''\n",
    "# Stemming sonrası verileri kelime listelerine dönüştürme\n",
    "stemmed_sentences = [text.split() for text in stemmed_df['text']]\n",
    "\n",
    "# Lemmatization sonrası verileri kelime listelerine dönüştürme\n",
    "lemmatized_sentences = [text.split() for text in lemmatized_df['text']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model parametreleri\n",
    "parameters = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec modellerini eğitme fonksiyonu\n",
    "def train_word2vec_models(sentences, params_list, prefix):\n",
    "    models_info = []\n",
    "    \n",
    "    for params in tqdm(params_list, desc=f\"Training {prefix} models\"):\n",
    "        model_type = params['model_type']\n",
    "        window = params['window']\n",
    "        vector_size = params['vector_size']\n",
    "        \n",
    "        # Model adını oluşturma\n",
    "        model_name = f\"{prefix}_{model_type}_win{window}_dim{vector_size}\"\n",
    "        \n",
    "        # Eğitim başlangıç zamanı\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Word2Vec modelini oluşturma ve eğitme\n",
    "        sg = 1 if model_type == 'skipgram' else 0  # sg=1: Skip-gram, sg=0: CBOW\n",
    "        model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window, \n",
    "                         min_count=2, workers=4, sg=sg)\n",
    "        \n",
    "        # Eğitim bitiş zamanı\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "        # Modeli kaydetme\n",
    "        model_path = os.path.join(models_dir, f\"{model_name}.model\")\n",
    "        model.save(model_path)\n",
    "        \n",
    "        # Model bilgilerini kaydetme\n",
    "        model_info = {\n",
    "            'model_name': model_name,\n",
    "            'model_type': model_type,\n",
    "            'window': window,\n",
    "            'vector_size': vector_size,\n",
    "            'training_time': training_time,\n",
    "            'model_size': os.path.getsize(model_path) / (1024*1024),  # MB cinsinden\n",
    "            'vocabulary_size': len(model.wv.key_to_index),\n",
    "            'model_path': model_path\n",
    "        }\n",
    "        \n",
    "        models_info.append(model_info)\n",
    "        \n",
    "    return models_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonrası veriler için Word2Vec modellerini eğitme\n",
    "'''\n",
    "stemmed_models_info = train_word2vec_models(stemmed_sentences, parameters, \"stemmed\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization sonrası veriler için Word2Vec modellerini eğitme\n",
    "'''\n",
    "lemmatized_models_info = train_word2vec_models(lemmatized_sentences, parameters, \"lemmatized\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word2Vec Model Bilgilerinin Gösterilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonrası Word2Vec model bilgilerini gösterme\n",
    "'''\n",
    "stemmed_models_df = pd.DataFrame(stemmed_models_info)\n",
    "print(\"Stemming sonrası Word2Vec modelleri:\")\n",
    "stemmed_models_df[['model_name', 'model_type', 'window', 'vector_size', 'training_time', 'model_size', 'vocabulary_size']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization sonrası Word2Vec model bilgilerini gösterme\n",
    "'''\n",
    "lemmatized_models_df = pd.DataFrame(lemmatized_models_info)\n",
    "print(\"Lemmatization sonrası Word2Vec modelleri:\")\n",
    "lemmatized_models_df[['model_name', 'model_type', 'window', 'vector_size', 'training_time', 'model_size', 'vocabulary_size']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Word2Vec Modellerinin Benzerlik Analizi\n",
    "\n",
    "Bu bölümde, eğitilen Word2Vec modellerini kullanarak kelime benzerliklerini inceleyeceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benzerlik analizi için örnek kelime seçme\n",
    "'''\n",
    "# Veri setinize göre önemli bir kelime seçin\n",
    "example_word = \"example\"  # Veri setinize göre değiştirin\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonrası modeller için benzerlik analizi\n",
    "'''\n",
    "print(\"Stemming sonrası modeller için benzerlik analizi:\")\n",
    "for model_info in stemmed_models_info:\n",
    "    model_path = model_info['model_path']\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    print(f\"\\nModel: {model_info['model_name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Örnek kelimeye en benzer 5 kelimeyi bulma\n",
    "        similar_words = model.wv.most_similar(example_word, topn=5)\n",
    "        print(f\"'{example_word}' kelimesine en benzer 5 kelime:\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"  {word}: {similarity:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{example_word}' kelimesi modelin kelime dağarcığında bulunmuyor.\")\n",
    "        # Alternatif bir kelime seçme\n",
    "        try:\n",
    "            # Modelin kelime dağarcığından rastgele bir kelime seçme\n",
    "            alt_word = list(model.wv.key_to_index.keys())[0]\n",
    "            similar_words = model.wv.most_similar(alt_word, topn=5)\n",
    "            print(f\"Alternatif olarak '{alt_word}' kelimesine en benzer 5 kelime:\")\n",
    "            for word, similarity in similar_words:\n",
    "                print(f\"  {word}: {similarity:.4f}\")\n",
    "        except:\n",
    "            print(\"Benzerlik analizi yapılamadı.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization sonrası modeller için benzerlik analizi\n",
    "'''\n",
    "print(\"Lemmatization sonrası modeller için benzerlik analizi:\")\n",
    "for model_info in lemmatized_models_info:\n",
    "    model_path = model_info['model_path']\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    print(f\"\\nModel: {model_info['model_name']}\")\n",
    "    \n",
    "    try:\n",
    "        # Örnek kelimeye en benzer 5 kelimeyi bulma\n",
    "        similar_words = model.wv.most_similar(example_word, topn=5)\n",
    "        print(f\"'{example_word}' kelimesine en benzer 5 kelime:\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"  {word}: {similarity:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{example_word}' kelimesi modelin kelime dağarcığında bulunmuyor.\")\n",
    "        # Alternatif bir kelime seçme\n",
    "        try:\n",
    "            # Modelin kelime dağarcığından rastgele bir kelime seçme\n",
    "            alt_word = list(model.wv.key_to_index.keys())[0]\n",
    "            similar_words = model.wv.most_similar(alt_word, topn=5)\n",
    "            print(f\"Alternatif olarak '{alt_word}' kelimesine en benzer 5 kelime:\")\n",
    "            for word, similarity in similar_words:\n",
    "                print(f\"  {word}: {similarity:.4f}\")\n",
    "        except:\n",
    "            print(\"Benzerlik analizi yapılamadı.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Başarısının Değerlendirilmesi\n",
    "\n",
    "Bu bölümde, eğitilen modellerin başarısını değerlendireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model başarısının değerlendirilmesi\n",
    "'''\n",
    "print(\"Model Başarısının Değerlendirilmesi:\")\n",
    "print(\"\\nStemming vs Lemmatization:\")\n",
    "print(\"- Stemming, kelimeleri köklerine indirgerken anlamsal bilgileri kaybedebilir.\")\n",
    "print(\"- Lemmatization, kelimeleri anlamsal köklerine indirgediği için daha anlamlı sonuçlar verebilir.\")\n",
    "\n",
    "print(\"\\nCBOW vs Skip-gram:\")\n",
    "print(\"- CBOW, bir kelimenin bağlamından (çevresindeki kelimelerden) o kelimeyi tahmin etmeye çalışır.\")\n",
    "print(\"- Skip-gram, bir kelimeden o kelimenin bağlamını (çevresindeki kelimeleri) tahmin etmeye çalışır.\")\n",
    "print(\"- Skip-gram genellikle nadir kelimeler için daha iyi performans gösterir, ancak eğitim süresi daha uzundur.\")\n",
    "\n",
    "print(\"\\nPencere Boyutu (Window Size):\")\n",
    "print(\"- Küçük pencere boyutu (2), yakın kelimelere daha fazla önem verir ve sözdizimsel ilişkileri daha iyi yakalar.\")\n",
    "print(\"- Büyük pencere boyutu (4), daha geniş bağlamı dikkate alır ve anlamsal ilişkileri daha iyi yakalar.\")\n",
    "\n",
    "print(\"\\nVektör Boyutu (Vector Size):\")\n",
    "print(\"- Küçük vektör boyutu (100), eğitim süresini kısaltır ancak temsil kapasitesi sınırlıdır.\")\n",
    "print(\"- Büyük vektör boyutu (300), daha zengin temsiller sağlar ancak eğitim süresi daha uzundur ve aşırı öğrenme riski vardır.\")\n",
    "\n",
    "print(\"\\nBeklenen En Başarılı Model:\")\n",
    "print(\"Lemmatization sonrası, Skip-gram, pencere boyutu 4 ve vektör boyutu 300 olan modelin en başarılı olması beklenir.\")\n",
    "print(\"Çünkü:\")\n",
    "print(\"- Lemmatization, anlamsal bilgileri koruduğu için daha iyi sonuçlar verir.\")\n",
    "print(\"- Skip-gram, nadir kelimeler için daha iyi performans gösterir.\")\n",
    "print(\"- Büyük pencere boyutu, daha geniş bağlamı dikkate alır.\")\n",
    "print(\"- Büyük vektör boyutu, daha zengin temsiller sağlar.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sonuç\n",
    "\n",
    "Bu notebook'ta, ön işleme adımlarından geçirilmiş metin verilerini TF-IDF ve Word2Vec yöntemleriyle vektörleştirdik. Her iki yöntem için de hem stemming hem de lemmatization sonucu elde edilen verileri kullandık. Word2Vec için farklı parametrelerle 16 farklı model eğittik ve bu modellerin performansını değerlendirdik."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
