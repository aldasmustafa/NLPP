{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doğal Dil İşleme Dersi - Ödev 1: Ön İşleme (Pre-processing)\n",
    "\n",
    "Bu notebook, metin tabanlı veri setine ön işleme adımlarının uygulanmasını içermektedir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gerekli Kütüphanelerin Yüklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# NLTK gerekli paketleri indirme\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Grafik ayarları\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ham Veriyi Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ham veriyi yükleme\n",
    "raw_data_dir = \"../data/raw/\"\n",
    "processed_data_dir = \"../data/processed/\"\n",
    "\n",
    "# Dizinlerin var olduğundan emin olalım\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "# Ham metin dosyasını okuma\n",
    "'''\n",
    "with open(os.path.join(raw_data_dir, \"raw_texts.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = f.readlines()\n",
    "    \n",
    "# Satır sonlarını temizleme\n",
    "texts = [text.strip() for text in texts]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ön İşleme Fonksiyonları\n",
    "\n",
    "Aşağıdaki ön işleme adımlarını uygulayacağız:\n",
    "1. HTML etiketleri ve özel karakterlerin temizlenmesi\n",
    "2. Tokenization (kelimelere ayırma)\n",
    "3. Lowercasing (küçük harfe dönüştürme)\n",
    "4. Stop word removal (gereksiz kelimelerin çıkarılması)\n",
    "5. Stemming (kelimelerin köklerine indirgenmesi)\n",
    "6. Lemmatization (kelimelerin anlamsal köklerine indirgenmesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML etiketlerini temizleme fonksiyonu\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Özel karakterleri ve noktalama işaretlerini temizleme fonksiyonu\n",
    "def clean_special_chars(text):\n",
    "    # Noktalama işaretlerini kaldırma\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', ' ', text)\n",
    "    # Fazla boşlukları temizleme\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Tokenization fonksiyonu\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Küçük harfe dönüştürme fonksiyonu\n",
    "def lowercase_text(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "# Stop word removal fonksiyonu\n",
    "def remove_stopwords(tokens, lang='english'):\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Stemming fonksiyonu\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Lemmatization fonksiyonu\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ön İşleme Adımlarının Uygulanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm metinlere ön işleme adımlarını uygulama\n",
    "'''\n",
    "preprocessed_texts = []\n",
    "stemmed_tokens_all = []\n",
    "lemmatized_tokens_all = []\n",
    "\n",
    "for text in texts:\n",
    "    # 1. HTML etiketlerini temizleme\n",
    "    text = clean_html(text)\n",
    "    \n",
    "    # 2. Özel karakterleri temizleme\n",
    "    text = clean_special_chars(text)\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # 4. Küçük harfe dönüştürme\n",
    "    tokens = lowercase_text(tokens)\n",
    "    \n",
    "    # 5. Stop word removal\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # 6a. Stemming\n",
    "    stemmed_tokens = stem_tokens(tokens)\n",
    "    stemmed_tokens_all.append(stemmed_tokens)\n",
    "    \n",
    "    # 6b. Lemmatization\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens)\n",
    "    lemmatized_tokens_all.append(lemmatized_tokens)\n",
    "    \n",
    "    # İşlenmiş metni kaydetme\n",
    "    preprocessed_texts.append({\n",
    "        'original': text,\n",
    "        'tokens': tokens,\n",
    "        'stemmed': stemmed_tokens,\n",
    "        'lemmatized': lemmatized_tokens\n",
    "    })\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Örnek Metin Üzerinde Ön İşleme Adımlarının Gösterilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek bir metin üzerinde ön işleme adımlarını gösterme\n",
    "'''\n",
    "example_text = texts[0]  # İlk metni örnek olarak alalım\n",
    "\n",
    "print(\"Orijinal metin:\")\n",
    "print(example_text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# HTML temizleme\n",
    "example_cleaned_html = clean_html(example_text)\n",
    "print(\"HTML etiketleri temizlendikten sonra:\")\n",
    "print(example_cleaned_html)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Özel karakterleri temizleme\n",
    "example_cleaned_special = clean_special_chars(example_cleaned_html)\n",
    "print(\"Özel karakterler temizlendikten sonra:\")\n",
    "print(example_cleaned_special)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Tokenization\n",
    "example_tokens = tokenize_text(example_cleaned_special)\n",
    "print(\"Tokenization sonrası:\")\n",
    "print(example_tokens)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Küçük harfe dönüştürme\n",
    "example_lowercase = lowercase_text(example_tokens)\n",
    "print(\"Küçük harfe dönüştürme sonrası:\")\n",
    "print(example_lowercase)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Stop word removal\n",
    "example_no_stopwords = remove_stopwords(example_lowercase)\n",
    "print(\"Stop word removal sonrası:\")\n",
    "print(example_no_stopwords)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Stemming\n",
    "example_stemmed = stem_tokens(example_no_stopwords)\n",
    "print(\"Stemming sonrası:\")\n",
    "print(example_stemmed)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Lemmatization\n",
    "example_lemmatized = lemmatize_tokens(example_no_stopwords)\n",
    "print(\"Lemmatization sonrası:\")\n",
    "print(example_lemmatized)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. İşlenmiş Verilerin CSV Dosyalarına Kaydedilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonucu elde edilen verileri CSV dosyasına kaydetme\n",
    "'''\n",
    "stemmed_data = []\n",
    "for i, tokens in enumerate(stemmed_tokens_all):\n",
    "    stemmed_data.append({\n",
    "        'document_id': i,\n",
    "        'text': ' '.join(tokens)\n",
    "    })\n",
    "\n",
    "stemmed_df = pd.DataFrame(stemmed_data)\n",
    "stemmed_df.to_csv(os.path.join(processed_data_dir, \"stemmed_data.csv\"), index=False)\n",
    "'''\n",
    "\n",
    "# Lemmatization sonucu elde edilen verileri CSV dosyasına kaydetme\n",
    "'''\n",
    "lemmatized_data = []\n",
    "for i, tokens in enumerate(lemmatized_tokens_all):\n",
    "    lemmatized_data.append({\n",
    "        'document_id': i,\n",
    "        'text': ' '.join(tokens)\n",
    "    })\n",
    "\n",
    "lemmatized_df = pd.DataFrame(lemmatized_data)\n",
    "lemmatized_df.to_csv(os.path.join(processed_data_dir, \"lemmatized_data.csv\"), index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Zipf Yasası Analizi (İşlenmiş Veriler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming sonrası Zipf yasası analizi\n",
    "'''\n",
    "# Tüm stemmed token'ları birleştirme\n",
    "all_stemmed_tokens = [token for tokens in stemmed_tokens_all for token in tokens]\n",
    "\n",
    "# Kelime frekanslarını hesaplama\n",
    "stemmed_word_counts = Counter(all_stemmed_tokens)\n",
    "\n",
    "# Zipf yasası grafiği için verileri hazırlama\n",
    "stemmed_word_freq = [(word, count) for word, count in stemmed_word_counts.items()]\n",
    "stemmed_word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "stemmed_ranks = np.arange(1, len(stemmed_word_freq) + 1)\n",
    "stemmed_frequencies = np.array([freq for word, freq in stemmed_word_freq])\n",
    "\n",
    "# Log-log grafiği çizme\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.loglog(stemmed_ranks, stemmed_frequencies, 'b.')\n",
    "plt.xlabel('Kelime Sıralaması (log)', fontsize=14)\n",
    "plt.ylabel('Kelime Frekansı (log)', fontsize=14)\n",
    "plt.title('Zipf Yasası Analizi (Stemming Sonrası)', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zipf yasası eğrisi (1/rank ilişkisi)\n",
    "plt.loglog(stemmed_ranks, stemmed_frequencies[0] / stemmed_ranks, 'r-', label='Zipf Yasası (1/rank)')\n",
    "plt.legend()\n",
    "plt.savefig('../data/processed/zipf_stemmed_data.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization sonrası Zipf yasası analizi\n",
    "'''\n",
    "# Tüm lemmatized token'ları birleştirme\n",
    "all_lemmatized_tokens = [token for tokens in lemmatized_tokens_all for token in tokens]\n",
    "\n",
    "# Kelime frekanslarını hesaplama\n",
    "lemmatized_word_counts = Counter(all_lemmatized_tokens)\n",
    "\n",
    "# Zipf yasası grafiği için verileri hazırlama\n",
    "lemmatized_word_freq = [(word, count) for word, count in lemmatized_word_counts.items()]\n",
    "lemmatized_word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "lemmatized_ranks = np.arange(1, len(lemmatized_word_freq) + 1)\n",
    "lemmatized_frequencies = np.array([freq for word, freq in lemmatized_word_freq])\n",
    "\n",
    "# Log-log grafiği çizme\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.loglog(lemmatized_ranks, lemmatized_frequencies, 'b.')\n",
    "plt.xlabel('Kelime Sıralaması (log)', fontsize=14)\n",
    "plt.ylabel('Kelime Frekansı (log)', fontsize=14)\n",
    "plt.title('Zipf Yasası Analizi (Lemmatization Sonrası)', fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zipf yasası eğrisi (1/rank ilişkisi)\n",
    "plt.loglog(lemmatized_ranks, lemmatized_frequencies[0] / lemmatized_ranks, 'r-', label='Zipf Yasası (1/rank)')\n",
    "plt.legend()\n",
    "plt.savefig('../data/processed/zipf_lemmatized_data.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Veri Boyutlarının Karşılaştırılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ham veri, stemming sonrası ve lemmatization sonrası veri boyutlarının karşılaştırılması\n",
    "'''\n",
    "# Ham veri boyutu\n",
    "raw_token_count = sum(len(text.split()) for text in texts)\n",
    "raw_unique_token_count = len(set(word for text in texts for word in text.split()))\n",
    "\n",
    "# Stemming sonrası veri boyutu\n",
    "stemmed_token_count = len(all_stemmed_tokens)\n",
    "stemmed_unique_token_count = len(stemmed_word_counts)\n",
    "\n",
    "# Lemmatization sonrası veri boyutu\n",
    "lemmatized_token_count = len(all_lemmatized_tokens)\n",
    "lemmatized_unique_token_count = len(lemmatized_word_counts)\n",
    "\n",
    "# Sonuçları tablo olarak gösterme\n",
    "data = {\n",
    "    'Veri Tipi': ['Ham Veri', 'Stemming Sonrası', 'Lemmatization Sonrası'],\n",
    "    'Toplam Token Sayısı': [raw_token_count, stemmed_token_count, lemmatized_token_count],\n",
    "    'Benzersiz Token Sayısı': [raw_unique_token_count, stemmed_unique_token_count, lemmatized_unique_token_count],\n",
    "    'Çıkarılan Token Yüzdesi': [0, (raw_token_count - stemmed_token_count) / raw_token_count * 100, \n",
    "                               (raw_token_count - lemmatized_token_count) / raw_token_count * 100],\n",
    "    'Benzersiz Token Azalma Yüzdesi': [0, (raw_unique_token_count - stemmed_unique_token_count) / raw_unique_token_count * 100,\n",
    "                                      (raw_unique_token_count - lemmatized_unique_token_count) / raw_unique_token_count * 100]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(data)\n",
    "comparison_df.set_index('Veri Tipi', inplace=True)\n",
    "comparison_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sonuç\n",
    "\n",
    "Bu notebook'ta, ham veri setine çeşitli ön işleme adımlarını uyguladık. Ön işleme adımları sonucunda elde edilen veriler, stemming ve lemmatization sonuçlarına göre iki ayrı CSV dosyasına kaydedildi. Ayrıca, her iki işlenmiş veri seti için Zipf yasası analizini gerçekleştirdik ve veri boyutlarını karşılaştırdık. Bir sonraki adımda, bu işlenmiş verileri kullanarak vektörleştirme işlemlerini gerçekleştireceğiz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
